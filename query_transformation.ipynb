{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8b56ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d04c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQQ_API_KEY\")\n",
    "os.environ[\"USER_AGENT\"] = \"my-rag-app/0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7aa0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afb9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths= (\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",),\n",
    "\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2544952",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50,\n",
    ")\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c360c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xoxo3\\AppData\\Local\\Temp\\ipykernel_19532\\3968552930.py:4: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embeddings = HuggingFaceBgeEmbeddings(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xoxo3\\OneDrive\\Desktop\\PYTHON\\5_hr_rag_course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = hf_embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a7b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatGroq(model = \"llama3-70b-8192\",temperature = 0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021eedc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are five alternative versions of the user question to retrieve relevant documents from a vector database:',\n",
       " '',\n",
       " 'What are the effects of descriptive and prescriptive approaches on annotation consistency and quality in natural language processing tasks?',\n",
       " '',\n",
       " 'How do different annotation paradigms, such as descriptive and prescriptive, influence the level of agreement among annotators in NLP tasks?',\n",
       " '',\n",
       " 'What is the relationship between annotation paradigm and annotation quality, and how do descriptive and prescriptive approaches compare in NLP tasks?',\n",
       " '',\n",
       " 'In what ways do descriptive and prescriptive annotation methods impact the reliability and validity of annotated data in natural language processing applications?',\n",
       " '',\n",
       " 'Can the choice of annotation paradigm, whether descriptive or prescriptive, affect the degree of inter-annotator agreement and overall quality of annotated data in NLP tasks?',\n",
       " '',\n",
       " 'These alternative questions offer different perspectives on the original question, which can help retrieve relevant documents from a vector database and overcome some of the limitations of distance-based similarity search.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(\"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0660573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" UNIQUE UNION OF RETRIEVAL DOCS \"\"\"\n",
    "\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705032b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xoxo3\\AppData\\Local\\Temp\\ipykernel_7172\\1524943120.py:9: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\"\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union \n",
    "docs = retrieval_chain.invoke({\"question\" : question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93e5c264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, the descriptive and prescriptive paradigms have different approaches to annotation quality and disagreement in NLP tasks.\\n\\nThe descriptive paradigm:\\n\\n* Encourages annotator subjectivity, trying to model many beliefs\\n* Embraces diversity and can help identify which entries are more subjective\\n* Is more aligned with standard NLP setup and easier to do QC by measuring disagreement or doing label aggregation\\n* Pros: can help to identify which entries are more subjective, embrace diversity, more aligned with standard NLP setup, and easier to do QC\\n* Cons: metrics like rater disagreement cannot be used to measure data quality or annotator performance, cannot be used for training models that are optimized for outputting one preset behavior, expensive and challenging to create high-quality annotation guidelines, and cannot capture an interpretable diversity of beliefs or consistently encode one specific belief\\n\\nThe prescriptive paradigm:\\n\\n* Discourages annotator subjectivity, trying to consistently apply one belief\\n* Is more aligned with standard NLP setup and easier to do QC by measuring disagreement or doing label aggregation\\n* Pros: more aligned with standard NLP setup and easier to do QC\\n* Cons: not mentioned explicitly in the text, but implied to be less effective in capturing diversity of opinions and more prone to errors or inconsistencies in annotation guidelines\\n\\nIn terms of disagreement, the descriptive paradigm acknowledges that disagreement is not always bad and can provide rich information, while the prescriptive paradigm aims to reduce disagreements caused by errors or poorly designed processes.\\n\\nOverall, the choice of paradigm depends on the specific goals and requirements of the NLP task, and a balanced approach that considers both diversity of opinions and consistency in annotation guidelines may be necessary to achieve high-quality annotations.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "     | prompt\n",
    "     | llm\n",
    "     | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7b815",
   "metadata": {},
   "source": [
    "RAG FUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5390d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada8efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatGroq(model = \"llama3-70b-8192\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ec0b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m     reranked_results = [\n\u001b[32m     15\u001b[39m         (loads(doc), score)\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fused_scores.items(), key = \u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m     ]\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reranked_results\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m retrieval_chain_rag_fusion = generate_queries | \u001b[43mretriever\u001b[49m.map() | reciprocal_rank_fusion\n\u001b[32m     21\u001b[39m docs = retrieval_chain_rag_fusion.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question})\n\u001b[32m     22\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs)\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "\n",
    "    for doc in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1/(rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key = lambda x:x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd2eeea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, the descriptive and prescriptive paradigms have different approaches to annotation quality and disagreement in NLP tasks.\\n\\nThe descriptive paradigm:\\n\\n* Encourages annotator subjectivity, trying to model many beliefs\\n* Embraces diversity and can help identify which entries are more subjective\\n* Is more aligned with standard NLP setup and easier to do quality control by measuring disagreement or doing label aggregation\\n* Pros: can help identify subjective entries, embraces diversity, and is easier to do QC\\n* Cons: metrics like rater disagreement cannot be used to measure data quality or annotator performance, and cannot be used for training models that are optimized for outputting one preset behavior\\n\\nThe prescriptive paradigm:\\n\\n* Discourages annotator subjectivity, trying to consistently apply one belief\\n* Is more aligned with standard NLP setup and easier to do quality control by measuring disagreement or doing label aggregation\\n* Pros: more aligned with standard NLP setup, easier to do QC\\n* Cons: does not embrace diversity, and may not capture an interpretable diversity of beliefs or consistently encode one specific belief\\n\\nIn terms of disagreement, the descriptive paradigm acknowledges that disagreement is not always bad and can provide rich information, while the prescriptive paradigm tries to reduce disagreements caused by errors or poorly designed processes.\\n\\nOverall, the choice of paradigm depends on the specific goals and requirements of the NLP task, and a balanced approach that considers both diversity and consistency may be necessary to achieve high-quality annotations.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\" : retrieval_chain_rag_fusion,\n",
    "     \"question\" : itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    |llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\" : question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d5195",
   "metadata": {},
   "source": [
    "DECOMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c5a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc79b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\"\n",
    "\n",
    "questions = generate_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd6551f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three sub-questions related to the input question:',\n",
       " '',\n",
       " '**Query 1:** ',\n",
       " 'What are the fundamental differences between descriptive and prescriptive annotation paradigms in NLP, and how do these differences influence annotator behavior and decision-making?',\n",
       " '',\n",
       " '**Query 2:** ',\n",
       " 'How do descriptive and prescriptive annotation approaches affect the consistency and reliability of annotations, particularly in tasks that involve subjective judgments, such as sentiment analysis or entity recognition?',\n",
       " '',\n",
       " '**Query 3:** ',\n",
       " 'What are the implications of descriptive and prescriptive paradigms on inter-annotator agreement and disagreement in NLP tasks, and how can these effects be mitigated through annotation guidelines, training, or other strategies?',\n",
       " '',\n",
       " 'These sub-questions can help to break down the original question into more manageable and specific topics, allowing for a more focused exploration of the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ede333",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f755dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question,answer):\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\n Anaswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")\n",
    "        }\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a63d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! Based on the provided context and background information, I understand that the original question is related to the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks.\\n\\nThe sub-questions mentioned are not explicitly provided, but based on the context, I can infer that they might be related to the following topics:\\n\\n1. What are the fundamental differences between descriptive and prescriptive annotation paradigms in NLP, and how do these differences influence annotator behavior and decision-making?\\n2. How do descriptive and prescriptive annotation approaches affect the consistency and reliability of annotations, particularly in tasks that involve subjective judgments, such as sentiment analysis or entity recognition?\\n3. What are the implications of descriptive and prescriptive paradigms on inter-annotator agreement and disagreement in NLP tasks, and how can these effects be mitigated through annotation guidelines, training, or other strategies?\\n\\nBy breaking down the original question into these more specific topics, we can explore the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks in a more focused and manageable way.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b219c6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for: These sub-questions can help to break down the original question into more manageable and specific topics, allowing for a more focused exploration of the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks.\n",
      "Context: I'm happy to help! Based on the provided context and background information, I understand that the original question is related to the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks.\n",
      "\n",
      "The sub-questions mentioned are not explicitly provided, but based on the context, I can infer that they might be related to the following topics:\n",
      "\n",
      "1. What are the fundamental differences between descriptive and prescriptive annotation paradigms in NLP, and how do these differences influence annotator behavior and decision-making?\n",
      "2. How do descriptive and prescriptive annotation approaches affect the consistency and reliability of annotations, particularly in tasks that involve subjective judgments, such as sentiment analysis or entity recognition?\n",
      "3. What are the implications of descriptive and prescriptive paradigms on inter-annotator agreement and disagreement in NLP tasks, and how can these effects be mitigated through annotation guidelines, training, or other strategies?\n",
      "\n",
      "By breaking down the original question into these more specific topics, we can explore the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks in a more focused and manageable way.\n",
      "\n",
      "The provided context and background information highlight the importance of considering annotator subjectivity, diversity of opinions, and the trade-offs between descriptive and prescriptive paradigms. For instance, the descriptive paradigm can help identify which entries are more subjective and embrace diversity, but it may not be suitable for training models that require a single, preset behavior. On the other hand, the prescriptive paradigm can ensure consistency in labeling, but it may not capture the diversity of beliefs and opinions.\n",
      "\n",
      "By exploring these topics and sub-questions, we can gain a deeper understanding of the impact of descriptive and prescriptive paradigms on annotation quality and disagreement in NLP tasks, and develop strategies to mitigate the effects of annotator subjectivity and disagreement.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running for:\", q)\n",
    "print(\"Context:\", rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415002cd",
   "metadata": {},
   "source": [
    "ANSWER INDIVIDUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb5ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e602a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "\n",
    "        retrieved_docs = retriever.get_relevant_documents\n",
    "        (sub_question)\n",
    "\n",
    "        answer = (\n",
    "            prompt_rag\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ).invoke\n",
    "\n",
    "        ({\"context\": retrieved_docs,\n",
    "          \"question\": sub_question\n",
    "        })\n",
    "        rag_results.append(answer)\n",
    "    return rag_results,sub_questions\n",
    "\n",
    "answers, questions = retrieve_and_rag(question,prompt_rag,generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cacd3c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided Q&A pairs, it appears that the descriptive and prescriptive paradigms have significant implications for annotation quality and disagreement in NLP tasks.\\n\\nThe descriptive paradigm, which focuses on describing the data as it is, tends to lead to more subjective and variable annotations. This is because annotators are not provided with clear guidelines or expectations, resulting in a wider range of possible interpretations and annotations. On the other hand, the prescriptive paradigm, which provides clear guidelines and expectations, tends to lead to more consistent and objective annotations.\\n\\nThe impact of these paradigms on annotation quality is significant. Descriptive paradigms can lead to lower-quality annotations due to the variability and subjectivity of the annotations. In contrast, prescriptive paradigms can lead to higher-quality annotations due to the consistency and objectivity of the annotations.\\n\\nIn terms of disagreement, the descriptive paradigm can lead to more disagreement among annotators due to the lack of clear guidelines and expectations. This can result in annotators having different interpretations of the data, leading to inconsistent annotations. On the other hand, the prescriptive paradigm can lead to less disagreement among annotators due to the clear guidelines and expectations, resulting in more consistent annotations.\\n\\nTo mitigate the effects of descriptive and prescriptive paradigms on annotation quality and disagreement, strategies such as providing clear guidelines and expectations, training annotators, and using annotation tools and platforms that facilitate consistency and objectivity can be employed.\\n\\nOverall, the choice of paradigm has significant implications for annotation quality and disagreement in NLP tasks. By understanding the strengths and weaknesses of each paradigm, researchers and practitioners can design annotation strategies that optimize annotation quality and minimize disagreement.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions,answers):\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    for i, (question,answer) in enumerate(zip(questions,answers),start = 1):\n",
    "        formatted_string += f\"Question {i}: {question}\\n Answer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions,answers)\n",
    "\n",
    "template =  \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfcef42",
   "metadata": {},
   "source": [
    "STEP - BACK IN RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fda9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "          \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",  \n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa70318c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do different approaches to language analysis affect the results of NLP tasks?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatGroq(model = \"llama3-70b-8192\", temperature = 0) | StrOutputParser()\n",
    "\n",
    "question = \"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\"\n",
    "\n",
    "generate_queries_step_back.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c40b3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The descriptive and prescriptive paradigms are two contrasting approaches to data annotation for subjective NLP tasks. The choice of paradigm significantly impacts annotation quality and disagreement in NLP tasks.\\n\\n**Descriptive Paradigm:**\\nIn the descriptive paradigm, annotators are encouraged to express their subjective opinions, and the goal is to model many beliefs. This approach:\\n\\n1. **Embraces diversity**: By allowing annotators to express their individual perspectives, the descriptive paradigm captures a diverse range of opinions.\\n2. **Identifies subjective entries**: This approach helps identify which entries are more subjective, which is essential in tasks where opinions vary widely.\\n3. **Aligns with standard NLP setup**: The descriptive paradigm is more aligned with standard NLP setup, making it easier to measure disagreement and perform label aggregation.\\n4. **Easier quality control**: Measuring disagreement and performing label aggregation are more straightforward in this paradigm, making quality control easier.\\n\\nHowever, the descriptive paradigm also has some limitations:\\n\\n1. **Cannot measure data quality or annotator performance**: Metrics like rater disagreement cannot be used to measure data quality or annotator performance in this paradigm.\\n2. **Not suitable for models optimized for one behavior**: The descriptive paradigm is not suitable for training models that are optimized for outputting one preset behavior.\\n3. **Expensive and challenging to create guidelines**: Creating high-quality annotation guidelines that can capture the diversity of opinions is expensive and challenging.\\n\\n**Prescriptive Paradigm:**\\nIn the prescriptive paradigm, annotators are discouraged from expressing their subjective opinions, and the goal is to consistently apply one belief. This approach:\\n\\n1. **Discourages annotator subjectivity**: By trying to consistently apply one belief, the prescriptive paradigm reduces annotator subjectivity.\\n2. **More suitable for models optimized for one behavior**: This paradigm is more suitable for training models that are optimized for outputting one preset behavior.\\n\\nHowever, the prescriptive paradigm also has some limitations:\\n\\n1. **Cannot capture diversity of beliefs**: This approach cannot capture the diversity of beliefs and opinions that exist in subjective NLP tasks.\\n2. **May not be suitable for tasks with diverse opinions**: The prescriptive paradigm may not be suitable for tasks where opinions vary widely, as it tries to enforce a single belief.\\n\\n**Impact on Annotation Quality and Disagreement:**\\nThe choice of paradigm significantly impacts annotation quality and disagreement. The descriptive paradigm is more suitable for tasks where opinions vary widely, while the prescriptive paradigm is more suitable for tasks where a single belief needs to be consistently applied.\\n\\nDisagreement is not always bad and can provide rich information about the diversity of opinions. However, disagreements caused by errors or poorly designed processes should be reduced. The descriptive paradigm can help identify which entries are more subjective, while the prescriptive paradigm can help reduce annotator subjectivity.\\n\\nIn conclusion, the choice of paradigm depends on the specific requirements of the NLP task and the desired outcome. Understanding the strengths and limitations of each paradigm is essential for achieving high-quality annotations and managing disagreement in subjective NLP tasks.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\": RunnableLambda(lambda x: x[\"question\"]),\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4709cf",
   "metadata": {},
   "source": [
    "HYDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30addbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a scientific paper passage that answers the question:\\n\\n**Title:** The Impact of Descriptive and Prescriptive Paradigms on Annotation Quality and Disagreement in NLP Tasks\\n\\n**Abstract:** Annotation is a crucial step in Natural Language Processing (NLP) tasks, as high-quality annotations are essential for training accurate models. However, annotation quality can be influenced by the paradigm adopted by annotators, which can be either descriptive or prescriptive. In this study, we investigate the impact of these two paradigms on annotation quality and disagreement in NLP tasks. Our results show that descriptive annotations, which focus on describing the natural language data, tend to exhibit higher quality and lower disagreement rates compared to prescriptive annotations, which aim to enforce specific guidelines or rules. We analyze the underlying causes of these differences and discuss their implications for NLP research and practice.\\n\\n**Introduction:** Annotation is a time-consuming and labor-intensive process in NLP, requiring human annotators to label or categorize text data according to specific guidelines or rules. The quality of annotations has a direct impact on the performance of NLP models, as noisy or inconsistent annotations can lead to biased or inaccurate models. Two dominant paradigms have emerged in annotation: descriptive and prescriptive. Descriptive annotations focus on describing the natural language data, without imposing specific guidelines or rules, whereas prescriptive annotations aim to enforce specific guidelines or rules to ensure consistency and standardization.\\n\\n**Methodology:** We conducted an experiment involving 100 annotators, who were randomly assigned to either a descriptive or prescriptive annotation paradigm. Each annotator was tasked with annotating a dataset of 500 sentences for a specific NLP task (e.g., sentiment analysis). We measured annotation quality using metrics such as inter-annotator agreement, annotation consistency, and accuracy. We also analyzed the sources of disagreement between annotators and the impact of paradigm on annotation time and effort.\\n\\n**Results:** Our results show that descriptive annotations exhibit higher quality and lower disagreement rates compared to prescriptive annotations. Specifically, we found that descriptive annotations achieved an average inter-annotator agreement of 85%, compared to 75% for prescriptive annotations. Additionally, descriptive annotations were more consistent, with an average consistency score of 90%, compared to 80% for prescriptive annotations. Furthermore, descriptive annotations required less time and effort, with an average annotation time of 30 minutes per sentence, compared to 45 minutes for prescriptive annotations.\\n\\n**Discussion:** Our findings suggest that descriptive annotations are more effective in capturing the nuances of natural language data, leading to higher quality and lower disagreement rates. This may be attributed to the fact that descriptive annotations allow annotators to rely on their linguistic intuition and contextual understanding, rather than adhering to rigid guidelines or rules. In contrast, prescriptive annotations may lead to over-standardization, which can result in loss of important contextual information and increased disagreement between annotators. Our results have significant implications for NLP research and practice, highlighting the need for a more nuanced approach to annotation that balances consistency with flexibility and contextual understanding.\\n\\n**Conclusion:** In conclusion, our study demonstrates that the choice of annotation paradigm has a significant impact on annotation quality and disagreement in NLP tasks. Descriptive annotations, which focus on describing the natural language data, tend to exhibit higher quality and lower disagreement rates compared to prescriptive annotations, which aim to enforce specific guidelines or rules. Our findings have important implications for the development of high-quality NLP models and highlight the need for a more nuanced approach to annotation that balances consistency with flexibility and contextual understanding.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\"\n",
    "\n",
    "generate_docs_for_retrieval.invoke({\"question\": question}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09da2595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='af6986a9-0fa3-4a9c-b148-31eb7cc1f0f3', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Often there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what’s important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive\\n\\n\\n\\n\\nDefinition\\nEncourage annotator subjectivity, trying to model many beliefs.\\nDiscourage annotator subjectivity, trying to consistently apply one belief.\\n\\n\\nPros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.'),\n",
       " Document(id='6c87210f-9b32-4bd3-8c26-14b6cec71ec0', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='The descriptive paradigm allows us to understand a number of important effects as well as to account for different perspectives. For example, annotator identity (e.g. African American, LGBTQ) is found to be a statistically significant factor in how they would label identify-related content as toxic (Goyal et al. 2022). Topics can be another main driver for diverse opinions. Wang et al. (2023) studied the human evaluation process of safety of an AI conversation system and compared results between labels by Trust & Safety (T&S) professionals and crowdsourcing annotators. They intentionally collected rich metadata associated with crowd annotators like demographic or behavior information. Comparing T&S expert labels and crowd annotations, they found that agreement rates vary across semantic topics and the level of severity:\\n\\nAgreement rate differs a lot across different topics; ranging from 0.96 on violence/gory to 0.25 on personal topics.\\nAgreement rates are higher on “extreme” and “benign” conversations, given four label options marking “benign”, “debatable”, “moderate” to “extreme”.\\n\\n\\n\\nCorrelations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)'),\n",
       " Document(id='13da0167-4b6e-43f9-b8ed-cbf36bf1d355', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Correlations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)\\n\\nZhang et al. (2023) proposed a taxonomy of rater disagreement to analyze the root causes. Among the listed causes, disagreement due to stochastic errors or inconsistency on the individual level should be avoided. In cases when a rater gives different labels to the same task when asked multiple times, some of those are most likely caused by human errors. Based on this intuition, the disagreement deconvolution method (Gordon et al. 2021) disentangles stable opinions from errors by anchoring each individual’s opinion to their own primary label and thus encouraging intra-rater consistency.\\n\\n\\nA taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\n\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:'),\n",
       " Document(id='4e734703-9fcc-4839-8d56-c8aabbe5f581', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Pros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.\\n\\n\\nCons\\n- Metrics like rater disagreement cannot be used to measure data quality or annotator performance; - Cannot be used for training models that are optimized for outputting one preset behavior.\\n- Expensive and challenging to create high-quality annotation guidelines, which can never be perfect, in practice;- Training annotators to get familiar with guideline in order to apply it properly is also challenging;- Cannot capture an interpretable diversity of beliefs or consistently encode one specific belief.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a55f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, the descriptive and prescriptive paradigms have different impacts on annotation quality and disagreement in NLP tasks.\\n\\nThe **Descriptive Paradigm**:\\n\\n* Encourages annotator subjectivity, trying to model many beliefs.\\n* Helps to identify which entries are more subjective.\\n* Embraces diversity.\\n* Is more aligned with standard NLP setup.\\n* Makes it easier to do quality control (QC) by measuring disagreement or doing label aggregation.\\n\\nThe **Prescriptive Paradigm**:\\n\\n* Discourages annotator subjectivity, trying to consistently apply one belief.\\n* Is more aligned with standard NLP setup.\\n* Makes it easier to do QC by measuring disagreement or doing label aggregation.\\n\\nIn terms of disagreement, the descriptive paradigm acknowledges that disagreement is not always bad and can provide rich information, whereas the prescriptive paradigm aims to reduce disagreements caused by errors or poorly designed processes.\\n\\nAdditionally, the context highlights that correlations between non-expert and expert annotations vary a lot across topics, and that disagreement deconvolution methods can be used to analyze the root causes of rater disagreement and disentangle stable opinions from errors.\\n\\nOverall, the choice of paradigm depends on the specific NLP task and the desired outcome. The descriptive paradigm is more suitable for tasks that require understanding diverse perspectives, while the prescriptive paradigm is more suitable for tasks that require consistent application of a specific belief or standard.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model = \"llama3-70b-8192\", temperature = 0)\n",
    "\n",
    "template =  \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_hyde_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_hyde_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
