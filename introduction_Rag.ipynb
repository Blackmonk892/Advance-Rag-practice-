{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6e7814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8df3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQQ_API_KEY\")\n",
    "os.environ[\"USER_AGENT\"] = \"my-rag-app/0.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "213ea297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778e0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths= (\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",),\n",
    "\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "045254d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Thinking about High-Quality Human Data\n",
      "    \n",
      "Date: February 5, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper ‚ÄúVox populi‚Äù) and nice feedback. üôè ]\n",
      "High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classificat\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cac369",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a02c0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xoxo3\\OneDrive\\Desktop\\PYTHON\\5_hr_rag_course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\xoxo3\\OneDrive\\Desktop\\PYTHON\\5_hr_rag_course\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xoxo3\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = hf_embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bbf29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, data quality refers to the quality of the data collected through machine learning techniques, whereas human data quality refers to the quality of data collected through human annotation, which involves attention to details and careful execution.\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatGroq(model = \"llama3-8b-8192\", temperature = 0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs,\"question\":\n",
    "    RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(rag_chain.invoke(\"What is the difference between data quality and human data quality?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1a509",
   "metadata": {},
   "source": [
    "INDEXING - IN RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "975d8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What food i like the most in mumbai?\"\n",
    "document = \"My favourite food in mumbai is vada pav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a68f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths = (\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = (\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c37937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e9d61a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}  \n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")\n",
    "\n",
    "query_result = hf_embeddings.embed_query(question)\n",
    "document_result = hf_embeddings.embed_documents([document])\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ed64550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.8929203021307338\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result[0])\n",
    "print(\"Cosine similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349dfec",
   "metadata": {},
   "source": [
    "VECTORSTORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27b2bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = splits,\n",
    "    embedding = hf_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4503ecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025E06416850>, search_kwargs={})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f8e10",
   "metadata": {},
   "source": [
    "RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb6ea821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xoxo3\\AppData\\Local\\Temp\\ipykernel_21204\\637557285.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "Often there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\n",
      "Disagreement is not always bad. We shou\n",
      "The descriptive paradigm allows us to understand a number of important effects as well as to account for different perspectives. For example, annotator identity (e.g. African American, LGBTQ) is found\n",
      "Pros\n",
      "- Can help to identify which entries are more subjective;- Embrace diversity\n",
      "- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.\n",
      "\n",
      "\n",
      "Cons\n",
      "Correlations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)\n",
      "\n",
      "Zhang et al. (2023) proposed a taxonomy of rater disagreement to analyze the root cau\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"How do descriptive and prescriptive paradigms impact annotation quality and disagreement in NLP tasks?\")\n",
    "print(\"Retrieved documents:\")\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:200])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9429d6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4077433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d89a5588-a0ae-4753-9381-71bd15f4c4d1', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Often there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what‚Äôs important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive\\n\\n\\n\\n\\nDefinition\\nEncourage annotator subjectivity, trying to model many beliefs.\\nDiscourage annotator subjectivity, trying to consistently apply one belief.\\n\\n\\nPros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.'),\n",
       " Document(id='d35c1049-4584-4d40-be25-8201dfdcabf1', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='The descriptive paradigm allows us to understand a number of important effects as well as to account for different perspectives. For example, annotator identity (e.g. African American, LGBTQ) is found to be a statistically significant factor in how they would label identify-related content as toxic (Goyal et al. 2022). Topics can be another main driver for diverse opinions. Wang et al. (2023) studied the human evaluation process of safety of an AI conversation system and compared results between labels by Trust & Safety (T&S) professionals and crowdsourcing annotators. They intentionally collected rich metadata associated with crowd annotators like demographic or behavior information. Comparing T&S expert labels and crowd annotations, they found that agreement rates vary across semantic topics and the level of severity:\\n\\nAgreement rate differs a lot across different topics; ranging from 0.96 on violence/gory to 0.25 on personal topics.\\nAgreement rates are higher on ‚Äúextreme‚Äù and ‚Äúbenign‚Äù conversations, given four label options marking ‚Äúbenign‚Äù, ‚Äúdebatable‚Äù, ‚Äúmoderate‚Äù to ‚Äúextreme‚Äù.\\n\\n\\n\\nCorrelations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)'),\n",
       " Document(id='707b9e17-c9e8-4954-abc5-025fcb42ae3c', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Pros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.\\n\\n\\nCons\\n- Metrics like rater disagreement cannot be used to measure data quality or annotator performance; - Cannot be used for training models that are optimized for outputting one preset behavior.\\n- Expensive and challenging to create high-quality annotation guidelines, which can never be perfect, in practice;- Training annotators to get familiar with guideline in order to apply it properly is also challenging;- Cannot capture an interpretable diversity of beliefs or consistently encode one specific belief.'),\n",
       " Document(id='ed4f1b2a-3ece-4f08-94d7-31e4226a6677', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/'}, page_content='Correlations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)\\n\\nZhang et al. (2023) proposed a taxonomy of rater disagreement to analyze the root causes. Among the listed causes, disagreement due to stochastic errors or inconsistency on the individual level should be avoided. In cases when a rater gives different labels to the same task when asked multiple times, some of those are most likely caused by human errors. Based on this intuition, the disagreement deconvolution method (Gordon et al. 2021) disentangles stable opinions from errors by anchoring each individual‚Äôs opinion to their own primary label and thus encouraging intra-rater consistency.\\n\\n\\nA taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\n\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
